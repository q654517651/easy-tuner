"""
ç»Ÿä¸€çš„Musubi-Tunerè®­ç»ƒå™¨ (FastAPI Backendç‰ˆæœ¬)
æ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹çš„LoRAè®­ç»ƒï¼ˆQwen-Image, Flux, Stable Diffusionç­‰ï¼‰
ä»src/easytuner/core/training/trainers/musubi_trainer.pyå®Œæ•´è¿ç§»
"""

import asyncio
import os
import time
import subprocess
import signal
import uuid
import json
import platform
import re
import psutil
from pathlib import Path
from typing import Callable, Optional, Dict, Any, List

from ....utils.logger import log_info, log_error, log_success, log_progress, log_warning
from ....utils.log_sink import LogSink
from ....core.exceptions import TrainingError
from ....utils.network_retry import NetworkRetryHelper
from ...config import get_config
from ...dataset.models import DatasetType
from ..models import BaseTrainingConfig, TrainingTask, TrainingState, get_model, list_models, build_cli_args, \
    build_toml_dict, dumps_toml


class MusubiTrainer:
    """ç»Ÿä¸€çš„Musubi-Tunerè®­ç»ƒå™¨ï¼ˆæ”¯æŒ workspace-based runtimeï¼‰"""
    _ACCELERATE_MODULE = "accelerate.commands.launch"

    def __init__(self, task_id: str, event_bus=None):
        from ...environment import get_paths
        import sys

        self.config = get_config()
        self._paths = get_paths()  # ç¼“å­˜ç¯å¢ƒè·¯å¾„
        self.task_id = task_id
        self.event_bus = event_bus
        self._proc: Optional[subprocess.Popen] = None
        self._cache_proc: Optional[subprocess.Popen] = None  # é¢„å¤„ç†è¿›ç¨‹
        self._cancelled = False  # å–æ¶ˆæ ‡å¿—
        self._id = uuid.uuid4().hex
        self._network_retry = NetworkRetryHelper(max_retries=2, retry_delay=3)
        self._last_logged_line: Optional[str] = None  # ç”¨äºè¿‡æ»¤é‡å¤æ—¥å¿—è¡Œ

        # âœ¨ æ–°æ¶æ„ï¼šä½¿ç”¨ç¯å¢ƒç®¡ç†å™¨ç»Ÿä¸€æä¾›çš„ Python è·¯å¾„ï¼ˆé¿å…é‡å¤é€»è¾‘ï¼‰
        self._python_exe = self._paths.runtime_python
        if not self._python_exe or not self._python_exe.exists():
            raise TrainingError(
                f"Runtime Python ä¸å­˜åœ¨: {self._python_exe}ï¼Œ"
                "è¯·åœ¨è®¾ç½®é¡µé¢é‡æ–°å®‰è£… Runtime ç¯å¢ƒ"
            )

        self._musubi_dir = self._paths.musubi_dir  # workspace/runtime/engines/musubi-tuner
        self._musubi_src = self._paths.musubi_src

        if not self._musubi_dir.exists():
            raise TrainingError(
                f"Musubi è®­ç»ƒå¼•æ“ä¸å­˜åœ¨: {self._musubi_dir}ï¼Œ"
                "è¯·åœ¨è®¾ç½®é¡µé¢é‡æ–°å®‰è£… Runtime ç¯å¢ƒ"
            )

        # æ³¨å†Œç¨‹åºé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°
        import atexit
        atexit.register(self._emergency_cleanup)

    def _get_task_dir(self, task: TrainingTask) -> Path:
        """è·å–ä»»åŠ¡ç›®å½•ï¼ˆæ”¯æŒæ–°çš„ task_id--name æ ¼å¼ï¼‰"""
        from ..manager import get_training_manager
        
        training_manager = get_training_manager()
        task_dir = training_manager.get_task_dir(task.id)
        
        if not task_dir:
            # å›é€€åˆ°æ—§æ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
            log_error(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡ç›®å½•ï¼Œä½¿ç”¨å›é€€è·¯å¾„: {task.id}")
            task_dir = Path(self.config.storage.workspace_root) / "tasks" / task.id
        
        return task_dir

    def _emit_event(self, event_type: str, data: Dict[str, Any]):
        """å‘é€äº‹ä»¶åˆ°äº‹ä»¶æ€»çº¿"""
        if self.event_bus:
            # ç¡®ä¿åŒ…å«task_id
            event_data = {"task_id": self.task_id, **data}
            try:
                # çº¿ç¨‹å®‰å…¨åœ°æŠ•é€’åˆ°ä¸»äº‹ä»¶å¾ªç¯ï¼ˆä¸é˜»å¡è®­ç»ƒçº¿ç¨‹ï¼‰
                self.event_bus.emit_threadsafe(event_type, event_data)
            except Exception as e:
                log_error(f"äº‹ä»¶å‘é€å¤±è´¥: {e}")

    def _emit_log(self, message: str, level: str = "info"):
        """å‘é€æ—¥å¿—äº‹ä»¶"""
        self._emit_event('training.log', {
            'message': message,
            'level': level,
            'timestamp': time.time()
        })

    def _emit_progress(self, **kwargs):
        """å‘é€è¿›åº¦æ›´æ–°äº‹ä»¶"""
        self._emit_event('training.progress', kwargs)

    @property
    def _PROJECT_ROOT(self) -> Path:
        """
        è·å–å·¥ä½œç›®å½•æ ¹ï¼ˆä»ç¯å¢ƒç®¡ç†å™¨ç¼“å­˜ï¼‰
        âœ¨ æ–°æ¶æ„ï¼šworkspace-based runtimeï¼Œæ‰€æœ‰è·¯å¾„ç›¸å¯¹äº workspace_root
        ç”¨äºè®¾ç½® Popen çš„ cwdï¼Œä¸ä¼šå†™å…¥ CLI å‚æ•°ã€‚
        """
        return self._paths.workspace_root

    # âœ¨ ç›´æ¥ä½¿ç”¨ç»å¯¹è·¯å¾„çš„ accelerate å‘½ä»¤ï¼ˆä¸å†ç›¸å¯¹åŒ–ï¼‰
    def _get_accelerate_cmd(self) -> List[str]:
        """è¿”å› accelerate å‘½ä»¤ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œè·¨å¹³å°ç¨³å®šï¼‰"""
        # ğŸ”§ ä¸ä½¿ç”¨ .resolve() ä»¥é¿å…åœ¨ Linux venv ä¸­è§£æç¬¦å·é“¾æ¥
        return [str(self._python_exe), "-m", self._ACCELERATE_MODULE]

    # ä»ä»»åŠ¡é…ç½®ç±»çš„ ClassVar å– 3 ä¸ªè„šæœ¬ï¼ˆç›¸å¯¹é¡¹ç›®æ ¹ï¼‰
    def _scripts_for_task(self, task) -> dict:
        """
        è¿”å›ï¼š
          - train: è®­ç»ƒè„šæœ¬çš„ç›¸å¯¹ workspace æ ¹è·¯å¾„ï¼ˆå¿…å¡«ï¼‰
          - cache_steps: [{name, script, args_template, enabled}]ï¼ˆä»æ¨¡å‹æ³¨å†Œè¡¨è¯»å–ï¼‰
        ä»…ä½¿ç”¨æ–°çš„ ModelSpec.cache_stepsï¼›ä¸å†è¯»å–æ—§çš„ script_cache_te/latentsã€‚
        """
        model_spec = get_model(task.training_type)
        # âœ¨ ä½¿ç”¨ç›¸å¯¹äº workspace_root çš„è·¯å¾„ï¼ˆå› ä¸º cwd æ˜¯ workspace_rootï¼‰
        base = self._musubi_src.relative_to(self._paths.workspace_root)

        def resolve(rel: str | None) -> str:
            if not rel:
                raise TrainingError("æœªå£°æ˜è„šæœ¬è·¯å¾„")
            rel = str(rel).replace("\\", "/")
            if rel.startswith("engines/"):
                p = Path(rel)
            elif "/" in rel:
                p = base / rel
            else:
                p = base / "musubi_tuner" / rel
            if p.suffix != ".py":
                p = p.with_suffix(".py")

            full_path = self._PROJECT_ROOT / p
            log_info(f"[è„šæœ¬è§£æ] ç›¸å¯¹è·¯å¾„: {p.as_posix()}")
            log_info(f"[è„šæœ¬è§£æ] é¡¹ç›®æ ¹: {self._PROJECT_ROOT}")
            log_info(f"[è„šæœ¬è§£æ] å®Œæ•´è·¯å¾„: {full_path}")
            log_info(f"[è„šæœ¬è§£æ] æ˜¯å¦å­˜åœ¨: {full_path.exists()}")

            if not full_path.exists():
                raise TrainingError(f"è„šæœ¬ä¸å­˜åœ¨: {p.as_posix()}ï¼ˆç›¸å¯¹é¡¹ç›®æ ¹ï¼‰")
            return p.as_posix()

        # è®­ç»ƒè„šæœ¬
        train_rel = model_spec.script_train
        if not train_rel:
            raise TrainingError(f"{model_spec.type_name} æœªå£°æ˜è®­ç»ƒè„šæœ¬")

        # ç¼“å­˜æ­¥éª¤ï¼ˆé€ä¸ªè§£æè„šæœ¬è·¯å¾„ï¼Œä¿ç•™æ¨¡æ¿ä¸ enabledï¼‰
        cache_steps = []
        for step in getattr(model_spec, "cache_steps", []) or []:
            cache_steps.append({
                "name": step.name,
                "script": resolve(step.script),
                "args_template": list(getattr(step, "args_template", [])),
                "enabled": getattr(step, "enabled", None),
            })

        return {
            "train": resolve(train_rel),
            "cache_steps": cache_steps,
        }

    # æŠŠä»»æ„è·¯å¾„è½¬æˆ"ç›¸å¯¹ workspace_root"çš„ POSIX å­—ç¬¦ä¸²ï¼ˆå¤±è´¥å°±æŠ›é”™ï¼Œä¸å›é€€ç»å¯¹ï¼‰
    def _rel_to_root_posix(self, p: Path) -> str:
        workspace_root = self._paths.workspace_root
        try:
            rel = os.path.relpath(p, workspace_root)
        except ValueError:
            raise TrainingError(f"è·¯å¾„ä¸åœ¨å·¥ä½œåŒºä¹‹ä¸‹ï¼Œæ— æ³•ä½¿ç”¨ç›¸å¯¹è·¯å¾„: {p}ï¼Œå·¥ä½œåŒº: {workspace_root}")

        # æ£€æŸ¥æ˜¯å¦è¶Šç•Œï¼ˆåŒ…å« ..ï¼‰
        if rel.startswith('..'):
            raise TrainingError(f"è·¯å¾„è¶Šç•Œï¼Œä¸åœ¨å·¥ä½œåŒºä¹‹ä¸‹: {p}ï¼Œå·¥ä½œåŒº: {workspace_root}")

        return rel.replace("\\", "/")


    def _parse_resolution_freeform(self, reso: str) -> tuple[int, int]:
        """
        å®½æ¾è§£æï¼šä»ä»»æ„å­—ç¬¦ä¸²é‡Œæå–æ•´æ•°ã€‚
        - å‘ç° 1 ä¸ªæ•´æ•° -> è§†ä¸ºæ­£æ–¹å½¢ (n, n)
        - å‘ç° â‰¥2 ä¸ªæ•´æ•° -> å–å‰ä¸¤ä¸ª (w, h)
        - æœªå‘ç°æ•´æ•° -> æŠ¥é”™
        """
        nums = re.findall(r"\d+", str(reso))
        if not nums:
            raise TrainingError(f"æ— æ•ˆçš„åˆ†è¾¨ç‡è¾“å…¥ï¼ˆè‡³å°‘éœ€è¦ä¸€ä¸ªæ•´æ•°æˆ–ä¸¤ä¸ªæ•´æ•°ï¼‰: {reso}")
        if len(nums) == 1:
            n = int(nums[0])
            return n, n
        return int(nums[0]), int(nums[1])

    def _to_posix(self, p: Path) -> str:
        return p.as_posix()

    def _bool(self, b: bool) -> str:
        return "true" if b else "false"

    def _create_dataset_config(self, task, preview_mode: bool = False) -> str:
        """
        ä½¿ç”¨æ³¨å†Œè¡¨å…ƒæ•°æ® + Trainer æ³¨å…¥çš„æ¯æ•°æ®é›†è¦†ç›–é¡¹ï¼Œç”Ÿæˆ dataset.toml
        - image_directory / video_directory / cache_directory / control_directory åœ¨æ­¤æ³¨å…¥ï¼›
        - å…¶å®ƒé”®ï¼ˆresolution/batch_size/enable_bucket/num_repeats/caption_extension ç­‰ï¼‰
          ç”± models.py çš„ build_toml_dict ä» config å…ƒæ•°æ®é‡Œè‡ªåŠ¨å†™å…¥ã€‚
        """
        # 1) è®¡ç®—æ•°æ®é›†è·¯å¾„ï¼ˆé¢„è§ˆæ¨¡å¼å¯è·³è¿‡ä¸¥æ ¼æ ¡éªŒï¼‰
        if preview_mode:
            # é¢„è§ˆæ¨¡å¼ï¼šè¿”å›å ä½è·¯å¾„ï¼ˆå‡è®¾æ˜¯imagesç›®å½•ï¼‰
            dataset_path = Path(self.config.storage.workspace_root) / "datasets" / (task.dataset_id or "preview") / "images"
            dataset_type = None
        else:
            dataset_path = self._resolve_dataset_path(task.dataset_id)
            # ä»æ•°æ®é›†ç›®å½•åæå–ç±»å‹ä¿¡æ¯
            dataset_type = self._detect_dataset_type(task.dataset_id)
            log_info(f"æ£€æµ‹åˆ°æ•°æ®é›†ç±»å‹: {dataset_type}")

        if preview_mode:
            # é¢„è§ˆæ¨¡å¼ï¼šä½¿ç”¨è™šæ‹Ÿè·¯å¾„ï¼Œä¸åˆ›å»ºå®é™…ç›®å½•
            training_dir = Path(self.config.storage.workspace_root) / "tasks" / "preview"
            cache_dir = training_dir / "cache"
        else:
            # å®é™…è®­ç»ƒï¼šä½¿ç”¨ç»Ÿä¸€çš„ä»»åŠ¡ç›®å½•ï¼ˆæ”¯æŒæ–°çš„ task_id--name æ ¼å¼ï¼‰
            training_dir = self._get_task_dir(task)
            cache_dir = training_dir / "cache"

            # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆè™½ç„¶åº”è¯¥å·²ç»ç”±TrainingManageråˆ›å»ºï¼‰
            cache_dir.mkdir(parents=True, exist_ok=True)

        # 3) è®¾ç½®æ•°æ®é›†å’Œç¼“å­˜è·¯å¾„åˆ°é…ç½®ä¸­ - ä½¿ç”¨ç»å¯¹è·¯å¾„
        # æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®ç›¸åº”çš„ç›®å½•å­—æ®µ
        if dataset_type == DatasetType.VIDEO.value:
            # è§†é¢‘æ•°æ®é›†ï¼šä½¿ç”¨ video_directory
            task.config.video_directory = str(dataset_path.resolve())
            task.config.image_video_directory = ""  # æ¸…ç©º image_directory
            log_info(f"è§†é¢‘æ•°æ®é›†è·¯å¾„ (video_directory): {task.config.video_directory}")
        else:
            # å›¾åƒæ•°æ®é›†ï¼šä½¿ç”¨ image_directory
            task.config.image_video_directory = str(dataset_path.resolve())
            task.config.video_directory = ""  # æ¸…ç©º video_directory
            log_info(f"å›¾åƒæ•°æ®é›†è·¯å¾„ (image_directory): {task.config.image_video_directory}")
        
        task.config.cache_directory = str(cache_dir.resolve())
        log_info(f"ç¼“å­˜ç›®å½•è·¯å¾„ (cache_directory): {task.config.cache_directory}")
        
        # 4) å¦‚æœæ˜¯æ§åˆ¶å›¾æ•°æ®é›†ï¼Œè®¾ç½® control_directory
        if dataset_type in [DatasetType.SINGLE_CONTROL_IMAGE.value, DatasetType.MULTI_CONTROL_IMAGE.value]:
            # æ§åˆ¶å›¾ç›®å½•ä¸åŸå›¾ç›®å½•åŒçº§
            controls_dir = dataset_path.parent / "controls"
            if controls_dir.exists():
                task.config.control_directory = str(controls_dir.resolve())
                log_info(f"æ§åˆ¶å›¾ç›®å½•è·¯å¾„ (control_directory): {task.config.control_directory}")
            else:
                log_warning(f"æ§åˆ¶å›¾ç›®å½•ä¸å­˜åœ¨: {controls_dir}")
                task.config.control_directory = ""
        else:
            # éæ§åˆ¶å›¾æ•°æ®é›†ï¼Œæ¸…ç©º control_directory
            task.config.control_directory = ""

        # 5) ç”Ÿæˆ TOML å†…å®¹
        toml_dict = build_toml_dict(task.config)
        toml_content = dumps_toml(toml_dict)

        # 6) è½ç›˜å¹¶è¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆä¾›è®­ç»ƒè„šæœ¬ --dataset_config ä½¿ç”¨ï¼‰
        toml_path = training_dir / "dataset.toml"
        if not preview_mode:
            # åªæœ‰éé¢„è§ˆæ¨¡å¼æ‰å†™å…¥æ–‡ä»¶
            toml_path.write_text(toml_content, encoding="utf-8")
        return self._rel_to_root_posix(toml_path)

    def _resolve_dataset_path(self, dataset_id: str, preview_mode: bool = False) -> Path:
        """è§£ææ•°æ®é›†è·¯å¾„ï¼ˆæ”¯æŒé¢„è§ˆæ¨¡å¼ï¼‰"""

        if preview_mode:
            # é¢„è§ˆæ¨¡å¼ï¼šè¿”å›å ä½è·¯å¾„ï¼Œé¿å…ä»»ä½•ä¾èµ–å¯¼å…¥ï¼ˆå‡è®¾æ˜¯imagesç›®å½•ï¼‰
            base = Path(self.config.storage.workspace_root) / "datasets" / (dataset_id or "preview")
            return base / "images"

        # è®­ç»ƒæ¨¡å¼ï¼šæŒ‰ç…§å®é™…çš„æ•°æ®é›†ç›®å½•ç»“æ„æŸ¥æ‰¾
        workspace_root = Path(self.config.storage.workspace_root)
        search_dirs = [
            workspace_root / "datasets" / "image_datasets",
            workspace_root / "datasets" / "control_image_datasets",
            workspace_root / "datasets"  # å‘åå…¼å®¹
        ]

        # æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®é›†ç›®å½• (æ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼)
        dataset_path = None
        for search_dir in search_dirs:
            if not search_dir.exists():
                continue
            for dir_path in search_dir.iterdir():
                if dir_path.is_dir():
                    # æ£€æŸ¥æ–°æ ¼å¼ {dataset_id}--{tag}--{name} æˆ–æ—§æ ¼å¼ {dataset_id}__{name}
                    if (dir_path.name.startswith(f"{dataset_id}--") or
                        dir_path.name.startswith(f"{dataset_id}__")):
                        dataset_path = dir_path
                        break
            if dataset_path:
                break

        if not dataset_path:
            raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: æœªæ‰¾åˆ°IDä¸º {dataset_id} çš„æ•°æ®é›†")

        # åœ¨æ•°æ®é›†ç›®å½•ä¸‹æŸ¥æ‰¾åŒ…å«å›¾åƒçš„å­ç›®å½•
        # æ”¯æŒï¼šimages/ï¼ˆæ™®é€šå›¾åƒæ•°æ®é›†ï¼‰ã€targets/ï¼ˆæ§åˆ¶å›¾æ•°æ®é›†çš„åŸå›¾ï¼‰ã€æ ¹ç›®å½•
        candidates = [dataset_path / "images", dataset_path / "targets", dataset_path]
        exts = {".jpg", ".jpeg", ".png", ".webp", ".bmp", ".gif", ".tiff"}

        for p in candidates:
            if p.exists():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ–‡ä»¶
                if any(f.is_file() and f.suffix.lower() in exts for f in p.glob("*")):
                    return p

        raise ValueError(
            f"åœ¨ {dataset_path} ä¸‹æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶ã€‚"
            f"è¯·å°†å›¾åƒæ”¾å…¥ 'images/'ï¼ˆæ™®é€šæ•°æ®é›†ï¼‰æˆ– 'targets/'ï¼ˆæ§åˆ¶å›¾æ•°æ®é›†ï¼‰ç›®å½•ä¸­ã€‚"
        )

    def _detect_dataset_type(self, dataset_id: str) -> Optional[str]:
        """æ£€æµ‹æ•°æ®é›†ç±»å‹ï¼ˆä»ç›®å½•åæ¨æ–­ï¼‰
        
        Returns:
            æ•°æ®é›†ç±»å‹å­—ç¬¦ä¸²ï¼Œå¦‚ 'single_control_image', 'multi_control_image', 'image', 'video'
            å¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å› None
        """
        workspace_root = Path(self.config.storage.workspace_root)
        search_dirs = [
            workspace_root / "datasets" / "image_datasets",
            workspace_root / "datasets" / "control_image_datasets",
            workspace_root / "datasets" / "video_datasets",
            workspace_root / "datasets"
        ]
        
        for search_dir in search_dirs:
            if not search_dir.exists():
                continue
            for dir_path in search_dir.iterdir():
                if dir_path.is_dir():
                    if (dir_path.name.startswith(f"{dataset_id}--") or
                        dir_path.name.startswith(f"{dataset_id}__")):
                        # ä»ç›®å½•åæå–ç±»å‹æ ‡ç­¾
                        from ...dataset.utils import parse_unified_dataset_dirname
                        _, dtype, _, _ = parse_unified_dataset_dirname(dir_path.name)
                        return dtype
        
        return None

    def _build_dynamic_args(self, config: BaseTrainingConfig, preview_mode: bool = False) -> List[str]:
        """ä½¿ç”¨æ–°çš„CLIå‚æ•°ç”Ÿæˆç³»ç»Ÿæ„å»ºå‘½ä»¤è¡Œå‚æ•°"""
        # ç›´æ¥ä½¿ç”¨æ–°æ¶æ„çš„build_cli_argså‡½æ•°ï¼ˆå·²å¤„ç†targetè¿‡æ»¤ï¼‰
        # å§‹ç»ˆç”Ÿæˆå®Œæ•´å‚æ•°åˆ—è¡¨ï¼Œç¡®ä¿é¢„è§ˆå’Œå®é™…æ‰§è¡Œä¸€è‡´
        return build_cli_args(config, force_emit_all=True)

    def _build_args_from_template(self, template: list[str], cfg, paths: dict[str, str]) -> list[str]:
        """
        å°† args_template ä¸­çš„å ä½ç¬¦ {key} ç”¨ cfg æˆ– paths çš„å€¼æ›¿æ¢ã€‚
        - ä¼˜å…ˆä» paths å–ï¼ˆå¦‚ {dataset_toml} / {cache_logs_dir}ï¼‰
        - å¦åˆ™ä» cfg å–ï¼ˆå¦‚ {vae_path} / {text_encoder_path} / {text_encoder_path_2}ï¼‰
        - ä¿ç•™æ™®é€šå­—é¢é‡ï¼ˆå¦‚ "--dataset_config"ï¼‰
        """
        out: list[str] = []
        for item in template:
            if isinstance(item, str) and item.startswith("{") and item.endswith("}"):
                key = item[1:-1]
                if key in paths:
                    out.append(str(paths[key]))
                else:
                    val = getattr(cfg, key, None)
                    if val is None:
                        raise TrainingError(f"ç¼ºå°‘å¿…é¡»çš„é…ç½®å­—æ®µ: {key}")
                    out.append(str(val))
            else:
                out.append(str(item))
        return out

    def _build_training_command(self, task: TrainingTask, dataset_config_rel: str, training_dir: Path,
                                preview_mode: bool = False) -> List[str]:
        """
        æ„å»ºè®­ç»ƒå‘½ä»¤ï¼ˆé¿å…å‚æ•°é‡å¤ï¼Œä½¿ç”¨å…ƒæ•°æ®ç³»ç»Ÿï¼‰
        - dataset_config_rel: ç›¸å¯¹é¡¹ç›®æ ¹çš„ dataset.toml è·¯å¾„ï¼ˆä¸Šä¸€æ­¥è¿”å›çš„å€¼ï¼‰
        - preview_mode: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼ï¼ˆå½±å“å‚æ•°æ˜¾ç¤ºï¼‰
        """
        config = task.config
        model_spec = get_model(task.training_type)

        # è„šæœ¬ï¼ˆç›¸å¯¹é¡¹ç›®æ ¹ï¼‰
        scripts = self._scripts_for_task(task)
        train_py_rel = scripts["train"]

        # è¾“å‡ºç›®å½•
        output_dir = (training_dir / "output").resolve()
        output_dir_rel = self._rel_to_root_posix(output_dir)
        if not preview_mode:
            output_dir.mkdir(parents=True, exist_ok=True)

        # âœ¨ ä½¿ç”¨è®­ç»ƒè„šæœ¬çš„ç»å¯¹è·¯å¾„ï¼ˆaccelerate æ”¯æŒç›´æ¥è¿è¡Œè„šæœ¬æ–‡ä»¶ï¼‰
        # é¿å… --module æ¨¡å¼ä¸‹ PYTHONPATH ç»§æ‰¿é—®é¢˜
        train_script_absolute = (self._PROJECT_ROOT / train_py_rel).resolve()
        log_info(f"[è®­ç»ƒè„šæœ¬] ç»å¯¹è·¯å¾„: {train_script_absolute}")
        log_info(f"[è®­ç»ƒè„šæœ¬] æ˜¯å¦å­˜åœ¨: {train_script_absolute.exists()}")

        if not train_script_absolute.exists():
            raise TrainingError(f"è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: {train_script_absolute}")

        # å›ºå®šå‚æ•°ï¼ˆç›´æ¥è¿è¡Œè„šæœ¬æ–‡ä»¶ï¼Œä¸ä½¿ç”¨ --moduleï¼‰
        cmd = [
            str(self._python_exe),  # ğŸ”§ ä¸ä½¿ç”¨ .resolve() ä»¥é¿å…åœ¨ Linux venv ä¸­è§£æç¬¦å·é“¾æ¥
            "-m",
            self._ACCELERATE_MODULE,
            ("--num_cpu_threads_per_process", "1"),
            ("--mixed_precision", "bf16"),
            str(train_script_absolute),  # âœ… ç›´æ¥ä½¿ç”¨è„šæœ¬ç»å¯¹è·¯å¾„
            ("--dataset_config", dataset_config_rel),
            ("--output_dir", output_dir_rel),
            ("--network_module", model_spec.network_module),
        ]

        # åŠ¨æ€å‚æ•°ï¼ˆæ¥è‡ªé…ç½®ï¼Œå·²è¿‡æ»¤target="cli"ï¼‰
        dynamic_args = self._build_dynamic_args(config, preview_mode=preview_mode)
        cmd.extend(dynamic_args)

        # é‡‡æ ·ç›¸å…³ï¼ˆå¦‚æœé…ç½®äº†sample_promptï¼‰
        self._add_sampling_args(cmd, config, training_dir, preview_mode=preview_mode)

        # æ—¥å¿—ç›®å½•
        log_dir = (training_dir / "logs").resolve()
        cmd.append(("--logging_dir", self._rel_to_root_posix(log_dir)))

        # å±•å¹³å‘½ä»¤ç”¨äºæ—¥å¿—æ˜¾ç¤º
        flat_cmd = self._flatten_command(cmd)
        log_info(f"è®­ç»ƒå‘½ä»¤: {' '.join(flat_cmd)}")
        return cmd

    def _add_sampling_args(self, cmd: List, config: BaseTrainingConfig, training_dir: Path, preview_mode: bool = False) -> None:
        """æ–¹æ¡ˆBï¼šç»Ÿä¸€çš„é‡‡æ ·å‚æ•°å¤„ç†é€»è¾‘"""
        # 1. æ£€æŸ¥é‡‡æ ·å¼€å…³
        sampling_enabled = getattr(config, "sampling_enabled", False)
        if not sampling_enabled:
            log_info("é‡‡æ ·åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡é‡‡æ ·è®¾ç½®")
            return

        # 2. è·å–é‡‡æ ·æç¤ºè¯
        sample_prompt = getattr(config, "sample_prompt", "").strip()
        if not sample_prompt:
            log_info("é‡‡æ ·æç¤ºè¯ä¸ºç©ºï¼Œè·³è¿‡é‡‡æ ·è®¾ç½®")
            return

        # 3. æ„å»ºé‡‡æ ·å†…å®¹ï¼ˆæ¸…ç†+é‡ç»„ï¼‰
        sampling_content = self._build_sampling_content(config, sample_prompt)

        # 4. å¤„ç†é‡‡æ ·æ–‡ä»¶è·¯å¾„
        sample_prompts_file = (training_dir / "sample_prompts.txt").resolve()

        if not preview_mode:
            # å®é™…æ¨¡å¼ï¼šåˆ›å»ºç›®å½•å¹¶å†™å…¥æ–‡ä»¶
            training_dir.mkdir(parents=True, exist_ok=True)
            sample_prompts_file.write_text(sampling_content, encoding="utf-8")

            # åˆ›å»ºé‡‡æ ·è¾“å‡ºç›®å½•
            sample_dir = (training_dir / "sample").resolve()
            sample_dir.mkdir(exist_ok=True)
        else:
            # é¢„è§ˆæ¨¡å¼ï¼šåªæ„å»ºè·¯å¾„ï¼Œä¸å®é™…åˆ›å»ºæ–‡ä»¶
            log_info(f"é¢„è§ˆæ¨¡å¼ï¼šé‡‡æ ·æ–‡ä»¶è·¯å¾„ {sample_prompts_file}")

        # 5. æ·»åŠ é‡‡æ ·CLIå‚æ•°
        cmd.extend([
            ("--sample_prompts", self._rel_to_root_posix(sample_prompts_file)),
        ])

        # 6. æ·»åŠ é‡‡æ ·é¢‘ç‡å‚æ•°ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
        sample_every_n_epochs = getattr(config, "sample_every_n_epochs", None)
        if sample_every_n_epochs and sample_every_n_epochs > 0:
            cmd.extend([("--sample_every_n_epochs", str(sample_every_n_epochs))])

        sample_at_first = getattr(config, "sample_at_first", False)
        if sample_at_first:
            cmd.append("--sample_at_first")

        log_info(f"é‡‡æ ·é…ç½®å·²ç”Ÿæˆ: {sampling_content[:100]}...")

    def _build_sampling_content(self, config: BaseTrainingConfig, prompt: str) -> str:
        """æ„å»ºå®Œæ•´çš„é‡‡æ ·å†…å®¹ï¼ˆæ”¯æŒå¤šè¡Œï¼Œæ™ºèƒ½æ¸…ç†ï¼‰"""
        lines = []

        # å¤„ç†å¤šè¡Œæç¤ºè¯
        for line in prompt.split('\n'):
            line = line.strip()
            if not line:
                continue

            # æ¸…ç†æ¯è¡Œä¸­çš„æ—§å‚æ•°
            clean_line = self._clean_sample_prompt(line)

            # ä¸ºæ¯è¡Œæ·»åŠ å½“å‰é…ç½®çš„å‚æ•°
            line_with_params = self._append_sampling_params(config, clean_line)
            lines.append(line_with_params)

        return '\n'.join(lines)

    def _clean_sample_prompt(self, prompt: str) -> str:
        """æ¸…ç†å•è¡Œæç¤ºè¯ä¸­çš„é‡‡æ ·å‚æ•°"""
        import re
        # ç§»é™¤ --w/--h/--s/--g/--d/--f ç­‰é‡‡æ ·å‚æ•°
        cleaned = re.sub(r'\s*--[whsgdf]\s+[\d.]+(?:\s|$)', ' ', prompt)
        return ' '.join(cleaned.split())

    def _append_sampling_params(self, config: BaseTrainingConfig, prompt: str) -> str:
        """ä¸ºæç¤ºè¯æ·»åŠ å½“å‰é…ç½®çš„é‡‡æ ·å‚æ•°"""
        params = []

        # å‚æ•°æ˜ å°„è¡¨ï¼ˆç¡®ä¿æ­£ç¡®çš„CLIå‚æ•°æ‹¼æ¥ï¼‰
        param_mappings = {
            'sample_width': '--w',      # å®½åº¦
            'sample_height': '--h',     # é«˜åº¦
            'sample_factor': '--f',     # å¸§æ•°
            'sample_steps': '--s',      # æ­¥æ•°
            'sample_guidance': '--g',   # æŒ‡å¯¼ç³»æ•°
            'sample_seed': '--d'        # ç§å­
        }

        # æ„å»ºå‚æ•°åˆ—è¡¨
        for field_name, cli_flag in param_mappings.items():
            value = getattr(config, field_name, None)
            if value is not None and str(value).strip() != "":
                params.extend([cli_flag, str(value).strip()])

        # ç»„åˆæç¤ºè¯å’Œå‚æ•°
        if params:
            return f"{prompt} {' '.join(params)}"
        else:
            return prompt

    def _flatten_command(self, cmd: List) -> List[str]:
        """å±•å¹³å‘½ä»¤ç”¨äºæ—¥å¿—æ˜¾ç¤º"""
        flat_cmd = []
        for item in cmd:
            if isinstance(item, tuple):
                flat_cmd.extend(item)
            elif isinstance(item, list):
                flat_cmd.extend(item)
            else:
                flat_cmd.append(str(item))
        return flat_cmd

    def _create_training_scripts(self, task: TrainingTask, dataset_config_rel: str, training_dir: Path) -> Dict[str, str]:
        """
        ç”Ÿæˆç®€æ´çš„ train.bat è„šæœ¬
        - æ¯è¡Œä¸€ä¸ªå‚æ•°ï¼Œä½¿ç”¨ ^ è¿æ¥
        - åˆ‡åˆ°é¡¹ç›®æ ¹ç›®å½•
        - è®¾ç½®å¿…è¦ç¯å¢ƒå˜é‡
        """
        # 1) å‘½ä»¤ï¼ˆç›¸å¯¹é¡¹ç›®æ ¹ï¼‰
        cmd = self._build_training_command(task, dataset_config_rel, training_dir, preview_mode=False)

        # 2) ä»è„šæœ¬ç›®å½•è·³å›é¡¹ç›®æ ¹çš„ç›¸å¯¹è·¯å¾„
        rel_to_root_from_script = os.path.relpath(self._PROJECT_ROOT, training_dir)
        rel_to_root_win = rel_to_root_from_script.replace("/", "\\")

        # 3) PYTHONPATH æŒ‡å‘ musubi æºç ç›®å½•ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        musubi_src_abs = str(self._musubi_src.resolve())

        # 4) æ„å»ºå¤šè¡Œå‘½ä»¤å­—ç¬¦ä¸²å’Œä¸€è¡Œå‘½ä»¤å­—ç¬¦ä¸²
        flat_cmd = self._flatten_command(cmd)

        # æ„å»ºå¤šè¡Œæ ¼å¼ï¼ˆWindowsæ‰¹å¤„ç†é£æ ¼ï¼‰- æ­£ç¡®çš„å‚æ•°å¯¹æ ¼å¼
        cmd_lines = []
        i = 0
        while i < len(flat_cmd):
            if i == 0:
                # ç¬¬ä¸€è¡Œï¼ˆPythonè„šæœ¬ï¼‰
                cmd_lines.append(f"{flat_cmd[i]} ^")
                i += 1
            elif i == len(flat_cmd) - 1:
                # æœ€åä¸€ä¸ªå‚æ•°ï¼ˆæ— ç»­è¡Œç¬¦ï¼‰
                cmd_lines.append(f"    {flat_cmd[i]}")
                i += 1
            elif flat_cmd[i].startswith('--'):
                # å‚æ•°åå¼€å¤´ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å€¼
                if i + 1 < len(flat_cmd) and not flat_cmd[i + 1].startswith('--'):
                    # å‚æ•°å¯¹ï¼š--param value
                    cmd_lines.append(f"    {flat_cmd[i]} {flat_cmd[i + 1]} ^")
                    i += 2
                else:
                    # å•ç‹¬çš„å¼€å…³å‚æ•°ï¼š--flag
                    if i == len(flat_cmd) - 1:
                        cmd_lines.append(f"    {flat_cmd[i]}")
                    else:
                        cmd_lines.append(f"    {flat_cmd[i]} ^")
                    i += 1
            else:
                # å…¶ä»–å‚æ•°
                if i == len(flat_cmd) - 1:
                    cmd_lines.append(f"    {flat_cmd[i]}")
                else:
                    cmd_lines.append(f"    {flat_cmd[i]} ^")
                i += 1

        # ç§»é™¤æœ€åä¸€è¡Œçš„ç»­è¡Œç¬¦
        if cmd_lines and cmd_lines[-1].endswith(' ^'):
            cmd_lines[-1] = cmd_lines[-1][:-2]

        multi_line_cmd = "\n".join(cmd_lines)
        command_line = " ".join(flat_cmd)

        # 5) Windows æ‰¹å¤„ç†ï¼ˆç®€æ´ç‰ˆæœ¬ï¼‰
        bat_content = f"""@echo off
cd /d "%~dp0{rel_to_root_win}"

set "PYTHONPATH={musubi_src_abs};%PYTHONPATH%"
set "PYTHONIOENCODING=utf-8"

{multi_line_cmd}
"""

        # 6) åªç”Ÿæˆ bat æ–‡ä»¶
        bat_path = training_dir / "train.bat"
        bat_path.write_text(bat_content, encoding="utf-8")

        log_info(f"è®­ç»ƒè„šæœ¬å·²ç”Ÿæˆ: {bat_path}")
        return {
            "bat_script": str(bat_path),
            "command_line": command_line  # ä¸€è¡Œå‘½ä»¤å­—ç¬¦ä¸²ï¼Œä¾¿äºæ˜¾ç¤ºå’Œä½¿ç”¨
        }

    def _run_cache_steps(self, task: TrainingTask, dataset_config_rel: str,
                         log_callback: Optional[Callable[[str], None]] = None) -> bool:
        """æ‰§è¡Œé¢„å¤„ç†ç¼“å­˜ï¼šæŒ‰ ModelSpec.cache_steps çš„é¡ºåºæ‰§è¡Œï¼›ä¸åšä»»ä½•è„šæœ¬åçŒœæµ‹ã€‚"""
        scripts = self._scripts_for_task(task)
        steps: list[dict] = scripts.get("cache_steps", [])

        if not steps:
            log_info("æ¨¡å‹æœªå£°æ˜ç¼“å­˜æ­¥éª¤ï¼Œè·³è¿‡é¢„å¤„ç†")
            return True

        # å·¥ä½œè·¯å¾„ä¸ä¸Šä¸‹æ–‡ (ä½¿ç”¨ç»Ÿä¸€ç›®å½•ç»“æ„ï¼Œæ”¯æŒæ–°çš„ task_id--name æ ¼å¼)
        training_dir = self._get_task_dir(task)
        cache_logs_dir = (training_dir / "cache").resolve()
        cache_logs_dir.mkdir(parents=True, exist_ok=True)

        paths_ctx = {
            "dataset_toml": dataset_config_rel,
            "cache_logs_dir": self._rel_to_root_posix(cache_logs_dir),
        }

        for step in steps:
            # æ¡ä»¶å¯ç”¨ï¼ˆè‹¥æ³¨å†Œäº† enabledï¼‰
            enabled_fn = step.get("enabled")
            if callable(enabled_fn):
                try:
                    if not enabled_fn(task.config):
                        log_info(f"è·³è¿‡ç¼“å­˜æ­¥éª¤ï¼ˆæœªå¯ç”¨ï¼‰: {step.get('name')}")
                        continue
                except Exception as e:
                    log_error(f"è¯„ä¼°ç¼“å­˜æ­¥éª¤æ˜¯å¦å¯ç”¨å¤±è´¥: {e}")
                    continue

            if self._cancelled:
                log_info("é¢„å¤„ç†è¢«å–æ¶ˆ")
                return False

            script_rel = step["script"]
            args_tmpl = step.get("args_template", [])

            # åŸºäºæ¨¡æ¿æ„å»ºæœ€ç»ˆå‚æ•°
            try:
                step_args = self._build_args_from_template(args_tmpl, task.config, paths_ctx)
            except Exception as e:
                err = f"æ„å»ºç¼“å­˜æ­¥éª¤å‚æ•°å¤±è´¥: {e}"
                log_error(err)
                if log_callback: log_callback(f"[é”™è¯¯] {err}")
                return False

            log_info(f"æ‰§è¡Œé¢„å¤„ç†æ­¥éª¤: {step.get('name')} -> {script_rel}")

            # âœ¨ ä½¿ç”¨ python -c é…åˆ runpy.run_module() è¿è¡Œï¼Œæ‰‹åŠ¨æ³¨å…¥ sys.path
            # script_rel æ ¼å¼ï¼šruntime/engines/musubi-tuner/src/musubi_tuner/qwen_image_cache_text_encoder_outputs.py
            # éœ€è¦è½¬æ¢ä¸ºæ¨¡å—è·¯å¾„ï¼šmusubi_tuner.qwen_image_cache_text_encoder_outputs

            # 1. æå–æ¨¡å—è·¯å¾„ï¼šä» .../src/ ä¹‹åçš„éƒ¨åˆ†
            script_rel_posix = script_rel.replace("\\", "/")
            if "/src/" in script_rel_posix:
                # æå– src/ ä¹‹åçš„éƒ¨åˆ†ï¼šmusubi_tuner/qwen_image_cache_text_encoder_outputs.py
                module_path_part = script_rel_posix.split("/src/", 1)[1]
                # ç§»é™¤ .py åç¼€å¹¶è½¬æ¢ä¸ºæ¨¡å—è·¯å¾„
                module_name = module_path_part.replace("/", ".").replace(".py", "")
                # ç»“æœï¼šmusubi_tuner.qwen_image_cache_text_encoder_outputs
            else:
                raise TrainingError(f"æ— æ³•è§£ææ¨¡å—è·¯å¾„: {script_rel}")

            # 2. è·å– musubi-tuner/src çš„ç»å¯¹è·¯å¾„ï¼ˆç”¨äº sys.path.insertï¼‰
            musubi_src_path = str(self._musubi_src.resolve())

            # 3. æ„å»º python -c å‘½ä»¤ï¼šæ‰‹åŠ¨æ³¨å…¥ sys.path å¹¶ä½¿ç”¨ runpy.run_module
            # æ ¼å¼: python.exe -c "import sys,runpy; sys.path.insert(0,r'è·¯å¾„'); runpy.run_module('æ¨¡å—å', run_name='__main__')" å‚æ•°...
            python_oneliner = (
                f"import sys,runpy; "
                f"sys.path.insert(0,r'{musubi_src_path}'); "
                f"runpy.run_module('{module_name}', run_name='__main__')"
            )

            cache_cmd = [
                str(self._python_exe),  # ğŸ”§ ä¸ä½¿ç”¨ .resolve() ä»¥é¿å…åœ¨ Linux venv ä¸­è§£æç¬¦å·é“¾æ¥
                "-c",
                python_oneliner,
                *step_args
            ]

            log_info(f"[ç¼“å­˜] ä½¿ç”¨ runpy æ–¹å¼æ‰§è¡Œ: {module_name}")
            log_info(f"[ç¼“å­˜] Pythonè·¯å¾„æ³¨å…¥: {musubi_src_path}")
            log_info(f"[ç¼“å­˜] Pythonå¯æ‰§è¡Œæ–‡ä»¶: {self._python_exe}")
            log_info(f"[ç¼“å­˜] å®Œæ•´å‘½ä»¤: {' '.join(cache_cmd)}")

            # 4. ç¯å¢ƒå˜é‡ï¼ˆä¿ç•™åŸºç¡€è®¾ç½®ï¼Œä¸å†ä¾èµ– PYTHONPATHï¼‰
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'

            try:
                # ç»Ÿä¸€ç”± LogSink è½ç›˜ä¸å¹¿æ’­ï¼ˆæ­¤å¤„ä¸ç›´æ¥å†™æ–‡ä»¶ï¼‰
                success = self._network_retry.run_with_retry(
                    command=cache_cmd,
                    cwd=str(self._PROJECT_ROOT),
                    env=env,
                    log_callback=log_callback,
                    timeout=1800,
                    log_file_path=None
                )
                if not success:
                    err = f"é¢„å¤„ç†å¤±è´¥: {step.get('name')} ({script_rel})"
                    log_error(err)
                    if log_callback: log_callback(f"[é”™è¯¯] {err}")
                    return False

                ok = f"é¢„å¤„ç†å®Œæˆ: {step.get('name')}"
                log_success(ok)
                if log_callback: log_callback(f"[å®Œæˆ] {ok}")

            except Exception as e:
                err = f"é¢„å¤„ç†å¼‚å¸¸: {e}"
                log_error(err)
                if log_callback: log_callback(f"[å¼‚å¸¸] {err}")
                return False

        return True

    def build_artifacts(self, task: TrainingTask, force: bool = False) -> Dict[str, Any]:
        """æ„å»ºè®­ç»ƒå·¥ä»¶ - ç”Ÿæˆæ‰€æœ‰å¿…è¦æ–‡ä»¶å¹¶è¿”å›scripts_info"""
        try:
            # éªŒè¯é…ç½®
            self._validate_config(task.config)

            # è®­ç»ƒç›®å½•ï¼ˆæ”¯æŒæ–°çš„ task_id--name æ ¼å¼ï¼‰
            training_dir = self._get_task_dir(task)

            # å¦‚æœä¸å¼ºåˆ¶é‡å»ºï¼Œä¸”æ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œç›´æ¥è¿”å›ç°æœ‰ä¿¡æ¯
            if not force and self.validate_artifacts(task):
                log_info(f"è®­ç»ƒå·¥ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡é‡å»º: {task.id}")
                return self._load_existing_scripts_info(task, training_dir)

            log_info(f"ç”Ÿæˆè®­ç»ƒå·¥ä»¶: {task.id}")

            # ç”Ÿæˆdataset.toml
            dataset_config_rel = self._create_dataset_config(task, preview_mode=False)

            # ç”Ÿæˆtrain.batå’Œè·å–command_line
            scripts_info = self._create_training_scripts(task, dataset_config_rel, training_dir)

            # æ„å»ºå®Œæ•´çš„scripts_info
            complete_scripts_info = {
                'command_line': scripts_info['command_line'],
                'bat_script': scripts_info['bat_script'],
                'dataset_config': dataset_config_rel,
                'ready': True,
                'generated_at': time.time()
            }

            log_success(f"è®­ç»ƒå·¥ä»¶ç”Ÿæˆå®Œæˆ: {task.id}")
            return complete_scripts_info

        except Exception as e:
            log_error(f"ç”Ÿæˆè®­ç»ƒå·¥ä»¶å¤±è´¥: {e}")
            raise TrainingError(f"ç”Ÿæˆè®­ç»ƒå·¥ä»¶å¤±è´¥: {e}")

    def validate_artifacts(self, task: TrainingTask) -> bool:
        """éªŒè¯è®­ç»ƒå·¥ä»¶æ˜¯å¦å®Œæ•´æœ‰æ•ˆ"""
        try:
            training_dir = self._get_task_dir(task)

            # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = [
                training_dir / "dataset.toml",
                training_dir / "train.bat"
            ]

            for file_path in required_files:
                if not file_path.exists():
                    log_info(f"å·¥ä»¶æ–‡ä»¶ç¼ºå¤±: {file_path}")
                    return False

            # æ£€æŸ¥é‡‡æ ·æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨é‡‡æ ·ï¼‰
            if hasattr(task.config, 'sampling_enabled') and task.config.sampling_enabled:
                sample_prompts_file = training_dir / "sample_prompts.txt"
                if not sample_prompts_file.exists():
                    log_info(f"é‡‡æ ·æ–‡ä»¶ç¼ºå¤±: {sample_prompts_file}")
                    return False

            return True

        except Exception as e:
            log_error(f"éªŒè¯å·¥ä»¶å¤±è´¥: {e}")
            return False

    def _load_existing_scripts_info(self, task: TrainingTask, training_dir: Path) -> Dict[str, Any]:
        """åŠ è½½ç°æœ‰çš„scripts_info"""
        try:
            # å°è¯•ä»task.jsonåŠ è½½ç°æœ‰çš„scripts_info
            task_file = training_dir / "task.json"
            if task_file.exists():
                with open(task_file, 'r', encoding='utf-8') as f:
                    task_data = json.load(f)
                    scripts_info = task_data.get('scripts_info')
                    if scripts_info and scripts_info.get('ready'):
                        return scripts_info

            # å¦‚æœæ²¡æœ‰ç°æœ‰ä¿¡æ¯ï¼Œé‡æ–°æ„å»ºåŸºæœ¬ä¿¡æ¯
            dataset_config_path = training_dir / "dataset.toml"
            bat_script_path = training_dir / "train.bat"

            # ä»batæ–‡ä»¶æ¨æ–­command_line
            command_line = ""
            if bat_script_path.exists():
                # ç®€åŒ–ï¼šè¯»å–batæ–‡ä»¶å†…å®¹å¹¶æå–å‘½ä»¤
                bat_content = bat_script_path.read_text(encoding='utf-8')
                # è¿™é‡Œå¯ä»¥åŠ å…¥æ›´å¤æ‚çš„è§£æé€»è¾‘
                command_line = "ä»ç°æœ‰è„šæœ¬é‡å»º"  # ä¸´æ—¶å ä½

            return {
                'command_line': command_line,
                'bat_script': str(bat_script_path),
                'dataset_config': self._rel_to_root_posix(dataset_config_path),
                'ready': True,
                'loaded_from_existing': True
            }

        except Exception as e:
            log_error(f"åŠ è½½ç°æœ‰scripts_infoå¤±è´¥: {e}")
            raise

    def prepare_training(self, task: TrainingTask) -> Dict[str, Any]:
        """å‡†å¤‡è®­ç»ƒç¯å¢ƒ - ç¡®ä¿å·¥ä»¶å­˜åœ¨å¹¶è¿”å›å®Œæ•´ä¿¡æ¯"""
        try:
            # éªŒè¯é…ç½®
            self._validate_config(task.config)

            # ä½¿ç”¨ç»Ÿä¸€ç›®å½•ç»“æ„ï¼ˆæ”¯æŒæ–°çš„ task_id--name æ ¼å¼ï¼‰
            training_dir = self._get_task_dir(task)

            # ç¡®ä¿å·¥ä»¶å­˜åœ¨ï¼ˆå¹‚ç­‰æ“ä½œï¼‰
            if not self.validate_artifacts(task):
                log_info("å·¥ä»¶ä¸å®Œæ•´ï¼Œé‡æ–°ç”Ÿæˆ...")
                scripts_info = self.build_artifacts(task, force=True)
            else:
                scripts_info = self._load_existing_scripts_info(task, training_dir)

            # è¿”å›å®Œæ•´çš„è®­ç»ƒä¿¡æ¯
            return {
                'training_dir': training_dir,
                'dataset_config': scripts_info['dataset_config'],
                'scripts_info': scripts_info,  # åŒ…å«command_lineï¼
                'log_file': training_dir / "train.log"
            }

        except Exception as e:
            log_error(f"å‡†å¤‡è®­ç»ƒå¤±è´¥: {e}")
            raise TrainingError(f"å‡†å¤‡è®­ç»ƒå¤±è´¥: {e}")

    def _validate_config(self, config: BaseTrainingConfig):
        """éªŒè¯è®­ç»ƒé…ç½®"""
        # å¯¹äºBaseTrainingConfigï¼Œæˆ‘ä»¬éªŒè¯åŸºæœ¬å­—æ®µå³å¯
        # æ¨¡å‹è·¯å¾„éªŒè¯ç§»åˆ°ä»»åŠ¡çº§åˆ«éªŒè¯
        if not hasattr(config, 'resolution') or not config.resolution:
            raise TrainingError("åˆ†è¾¨ç‡å‚æ•°æ— æ•ˆ")

        if not hasattr(config, 'batch_size') or config.batch_size <= 0:
            raise TrainingError("æ‰¹å¤§å°å¿…é¡»å¤§äº0")

        # éªŒè¯æ¨¡å‹è·¯å¾„ (å¯¹äºQwen-Image) - åªåœ¨è·¯å¾„éç©ºæ—¶éªŒè¯
        if hasattr(config, 'dit_path') and config.dit_path and config.dit_path.strip():
            if not os.path.exists(config.dit_path):
                raise TrainingError(f"DiTæ¨¡å‹è·¯å¾„æ— æ•ˆ: {config.dit_path}")
        if hasattr(config, 'vae_path') and config.vae_path and config.vae_path.strip():
            if not os.path.exists(config.vae_path):
                raise TrainingError(f"VAEæ¨¡å‹è·¯å¾„æ— æ•ˆ: {config.vae_path}")
        if hasattr(config, 'text_encoder_path') and config.text_encoder_path and config.text_encoder_path.strip():
            if not os.path.exists(config.text_encoder_path):
                raise TrainingError(f"Text Encoderè·¯å¾„æ— æ•ˆ: {config.text_encoder_path}")

    def run_training(self,
                     task: TrainingTask,
                     progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
                     log_callback: Optional[Callable[[str], None]] = None) -> bool:
        """è¿è¡Œè®­ç»ƒ"""
        sink: Optional[LogSink] = None
        try:
            # é‡ç½®å–æ¶ˆæ ‡å¿—
            self._cancelled = False

            # ç»Ÿä¸€æ—¥å¿—å†™å…¥/å¹¿æ’­
            sink = LogSink(self.task_id, self.event_bus, self.config.storage.workspace_root)

            # å‡†å¤‡è®­ç»ƒ - ä¸ç›´æ¥ä¿®æ”¹task.stateï¼Œé€šè¿‡äº‹ä»¶é€šçŸ¥
            sink.write_line("å‡†å¤‡è®­ç»ƒç¯å¢ƒ...", phase='cache')
            # å‡†å¤‡é˜¶æ®µä»ç„¶ä¿æŒRUNNINGçŠ¶æ€ï¼Œä¸å†å•ç‹¬è®¾ç½®preparingçŠ¶æ€

            training_info = self.prepare_training(task)

            # æ‰§è¡Œé¢„å¤„ç†ç¼“å­˜æ­¥éª¤
            log_info("å¼€å§‹é¢„å¤„ç†ç¼“å­˜æ­¥éª¤...")
            if log_callback:
                log_callback("å¼€å§‹é¢„å¤„ç†ç¼“å­˜æ­¥éª¤...")

            # ç¼“å­˜é˜¶æ®µï¼šé€šè¿‡ LogSink ç»Ÿä¸€å†™å…¥ä¸å¹¿æ’­
            cache_success = self._run_cache_steps(
                task,
                training_info['dataset_config'],
                log_callback=(lambda line: sink.write_line(line, phase='cache'))
            )
            if not cache_success:
                # ä¸ç›´æ¥ä¿®æ”¹task.stateï¼Œé€šè¿‡å›è°ƒé€šçŸ¥
                error_msg = "é¢„å¤„ç†ç¼“å­˜å¤±è´¥"
                sink.write_line(error_msg, phase='cache', level='error')
                if progress_callback:
                    progress_callback({"state": "failed", "error": error_msg})
                return False

            # å¼€å§‹è®­ç»ƒ - å…ˆè¾“å‡ºæ¬¢è¿æ¨ªå¹…
            banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                              â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ    â–ˆâ–ˆ        â•‘
â•‘     â–ˆâ–ˆ      â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ       â–ˆâ–ˆ  â–ˆâ–ˆ         â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ          â•‘
â•‘     â–ˆâ–ˆ      â–ˆâ–ˆ   â–ˆâ–ˆ      â–ˆâ–ˆ    â–ˆâ–ˆ           â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ           â•‘
â•‘                                              â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â•‘
â•‘        â–ˆâ–ˆ    â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ           â•‘
â•‘        â–ˆâ–ˆ    â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ  â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        â•‘
â•‘        â–ˆâ–ˆ    â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ  â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ           â•‘
â•‘        â–ˆâ–ˆ     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â•‘
â•‘                                              â•‘
â•‘       ğŸš€ AI Model Training Made Easy ğŸš€      â•‘
â•‘                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
            # è¾“å‡ºæ¨ªå¹…åˆ°æ—¥å¿—å’ŒWebSocket
            for line in banner.strip().split('\n'):
                sink.write_line(line, phase='train')
            
            sink.write_line("", phase='train')  # ç©ºè¡Œ
            sink.write_line("å¼€å§‹æ­£å¼è®­ç»ƒ...", phase='train')
            if progress_callback:
                progress_callback({"state": "running"})

            log_info(f"å¼€å§‹è®­ç»ƒ: {task.name}")
            if log_callback:
                log_callback(f"å¼€å§‹è®­ç»ƒ: {task.name}")

            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹ - ç›´æ¥é‡æ–°æ„å»ºå‘½ä»¤é¿å…å­—ç¬¦ä¸²åˆ†å‰²é—®é¢˜
            training_dir = training_info['training_dir']
            dataset_config = training_info['dataset_config']
            cmd_list = self._build_training_command(task, dataset_config, training_dir, preview_mode=False)
            cmd = self._flatten_command(cmd_list)

            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°musubi_tuneræ¨¡å—
            env = os.environ.copy()
            # âœ¨ ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ˆworkspace-based runtimeï¼‰
            env['PYTHONPATH'] = str(self._musubi_src.resolve()) + os.pathsep + env.get('PYTHONPATH', '')
            env['PYTHONIOENCODING'] = 'utf-8'

            # ä½¿ç”¨ç½‘ç»œé‡è¯•é€»è¾‘å¯åŠ¨è®­ç»ƒï¼ˆé€ä¼  log_sinkï¼‰
            return self._run_training_with_retry(task, cmd, env, progress_callback, log_callback, sink)

        except Exception as e:
            # ä¸ç›´æ¥ä¿®æ”¹task.stateï¼Œé€šè¿‡å›è°ƒé€šçŸ¥
            error_msg = str(e)
            if sink is not None:
                sink.write_line(f"è®­ç»ƒå¼‚å¸¸: {error_msg}", phase='train', level='error')
            else:
                self._emit_log(f"è®­ç»ƒå¼‚å¸¸: {error_msg}", "error")
            log_error(f"è®­ç»ƒå¤±è´¥: {error_msg}")
            if progress_callback:
                progress_callback({
                    "state": "failed",
                    "error": error_msg
                })
            return False
        finally:
            try:
                if sink is not None:
                    sink.close()
            except Exception:
                pass

    def _run_training_with_retry(self,
                                 task: TrainingTask,
                                 cmd: List[str],
                                 env: Dict[str, str],
                                 progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
                                 log_callback: Optional[Callable[[str], None]] = None,
                                 log_sink: Optional[LogSink] = None) -> bool:
        """ä½¿ç”¨ç½‘ç»œé‡è¯•é€»è¾‘è¿è¡Œè®­ç»ƒ"""
        last_error = None

        for attempt in range(self._network_retry.max_retries + 1):
            # æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
            if self._cancelled:
                if log_sink is not None:
                    log_sink.write_line("è®­ç»ƒè¢«å–æ¶ˆ", phase='train', level='warning')
                else:
                    self._emit_log("è®­ç»ƒè¢«å–æ¶ˆ", "warning")
                log_info("è®­ç»ƒè¢«å–æ¶ˆ")
                return False

            # è®¾ç½®é•œåƒç«™
            if attempt < len(self._network_retry.HF_MIRRORS):
                self._network_retry._set_hf_mirror(self._network_retry.HF_MIRRORS[attempt])

            # è®°å½•å°è¯•ä¿¡æ¯
            mirror_info = f"(å°è¯• {attempt + 1}/{self._network_retry.max_retries + 1}"
            if attempt < len(self._network_retry.HF_MIRRORS):
                mirror_info += f", é•œåƒ: {self._network_retry.HF_MIRRORS[attempt]}"
            mirror_info += ")"

            start_msg = f"å¯åŠ¨è®­ç»ƒ {mirror_info}: {task.name}"
            log_info(start_msg)
            if log_sink is not None:
                log_sink.write_line(start_msg, phase='train')
            if log_callback:
                log_callback(start_msg)

            try:
                # å¤åˆ¶ç¯å¢ƒå˜é‡å¹¶åº”ç”¨å½“å‰é•œåƒè®¾ç½®
                current_env = env.copy()
                if 'HF_ENDPOINT' in os.environ:
                    current_env['HF_ENDPOINT'] = os.environ['HF_ENDPOINT']

                # ğŸ” è°ƒè¯•ï¼šæ‰“å° PYTHONPATH ç¡®è®¤æ˜¯å¦æ­£ç¡®è®¾ç½®
                log_info(f"[ç¯å¢ƒå˜é‡] PYTHONPATH = {current_env.get('PYTHONPATH', '(æœªè®¾ç½®)')}")
                log_info(f"[å·¥ä½œç›®å½•] cwd = {self._PROJECT_ROOT}")
                log_info(f"[å‘½ä»¤] {' '.join(cmd[:5])}...")  # åªæ‰“å°å‰5ä¸ªå‚æ•°

                # åˆ›å»ºè¿›ç¨‹
                self._proc = subprocess.Popen(
                    cmd,
                    cwd=str(self._PROJECT_ROOT),
                    env=current_env,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True, encoding='utf-8', errors='replace', bufsize=1
                )

                # ç›‘æ§è®­ç»ƒè¿›åº¦ï¼ˆç»Ÿä¸€ç”± LogSink è´Ÿè´£å†™å…¥/å¹¿æ’­ï¼‰
                training_result = self._monitor_training(task, progress_callback, log_callback, None, log_sink)

                # æ£€æŸ¥æ˜¯å¦åœ¨ç›‘æ§è¿‡ç¨‹ä¸­è¢«å–æ¶ˆ
                if self._cancelled:
                    log_info("è®­ç»ƒåœ¨ç›‘æ§è¿‡ç¨‹ä¸­è¢«å–æ¶ˆ")
                    if log_sink is not None:
                        log_sink.write_line("è®­ç»ƒåœ¨ç›‘æ§è¿‡ç¨‹ä¸­è¢«å–æ¶ˆ", phase='train', level='warning')
                    return False

                # æ£€æŸ¥è®­ç»ƒç»“æœ
                if training_result:
                    log_success(f"è®­ç»ƒæˆåŠŸå®Œæˆ {mirror_info}")
                    return True

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€æ˜¯å¦ä¸ºå–æ¶ˆ
                if hasattr(task, 'state') and task.state == TrainingState.CANCELLED:
                    log_info("è®­ç»ƒä»»åŠ¡å·²è¢«å–æ¶ˆï¼Œåœæ­¢é‡è¯•")
                    if log_sink is not None:
                        log_sink.write_line("è®­ç»ƒä»»åŠ¡å·²è¢«å–æ¶ˆï¼Œåœæ­¢é‡è¯•", phase='train', level='warning')
                    return False

                # è®­ç»ƒå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç½‘ç»œé”™è¯¯
                error_output = getattr(task, 'error_message', '')
                if not self._network_retry._is_network_error(error_output):
                    # éç½‘ç»œé”™è¯¯ï¼Œä¸é‡è¯•
                    log_error(f"éç½‘ç»œé”™è¯¯ï¼Œåœæ­¢é‡è¯•: {error_output}")
                    return False

                last_error = f"è®­ç»ƒå¤±è´¥ï¼ˆç½‘ç»œé—®é¢˜ï¼‰: {error_output}"
                log_error(f"ç½‘ç»œé”™è¯¯ {mirror_info}: {last_error}")
                net_msg = f"[ç½‘ç»œé”™è¯¯] {mirror_info}: æ£€æµ‹åˆ°ç½‘ç»œé—®é¢˜"
                if log_sink is not None:
                    log_sink.write_line(net_msg, phase='train', level='error')
                if log_callback:
                    log_callback(net_msg)

            except Exception as e:
                last_error = f"è®­ç»ƒå¼‚å¸¸: {str(e)}"
                log_error(f"å¼‚å¸¸ {mirror_info}: {last_error}")
                exc_msg = f"[å¼‚å¸¸] {mirror_info}: {last_error}"
                if log_sink is not None:
                    log_sink.write_line(exc_msg, phase='train', level='error')
                if log_callback:
                    log_callback(exc_msg)

            finally:
                # ğŸ”§ å¢å¼ºè¿›ç¨‹æ¸…ç†é€»è¾‘ï¼Œç¡®ä¿å­è¿›ç¨‹ä¸é˜»å¡
                if self._proc:
                    try:
                        if self._proc.poll() is None:
                            log_info("ç»ˆæ­¢è®­ç»ƒè¿›ç¨‹...")
                            self._proc.terminate()
                            try:
                                self._proc.wait(timeout=10)
                                log_info("è®­ç»ƒè¿›ç¨‹å·²æ­£å¸¸ç»ˆæ­¢")
                            except subprocess.TimeoutExpired:
                                log_warning("è¿›ç¨‹æœªå“åº”terminateï¼Œå¼ºåˆ¶kill")
                                self._proc.kill()
                                self._proc.wait(timeout=5)
                                log_info("è®­ç»ƒè¿›ç¨‹å·²å¼ºåˆ¶ç»ˆæ­¢")
                        
                        # ğŸ”§ å…³é—­è¾“å‡ºæµï¼Œé˜²æ­¢ç®¡é“é˜»å¡
                        if self._proc.stdout and not self._proc.stdout.closed:
                            try:
                                self._proc.stdout.close()
                            except:
                                pass
                        if self._proc.stderr and not self._proc.stderr.closed:
                            try:
                                self._proc.stderr.close()
                            except:
                                pass
                                
                    except Exception as e:
                        log_error(f"æ¸…ç†è¿›ç¨‹å¤±è´¥: {e}")
                    finally:
                        self._proc = None

            # å†æ¬¡æ£€æŸ¥æ˜¯å¦åœ¨å¤„ç†è¿‡ç¨‹ä¸­è¢«å–æ¶ˆ
            if self._cancelled:
                log_info("è®­ç»ƒè¢«å–æ¶ˆï¼Œåœæ­¢é‡è¯•")
                if log_sink is not None:
                    log_sink.write_line("è®­ç»ƒè¢«å–æ¶ˆï¼Œåœæ­¢é‡è¯•", phase='train', level='warning')
                return False

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt < self._network_retry.max_retries:
                # åœ¨ç­‰å¾…é‡è¯•å‰å†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€
                if self._cancelled:
                    log_info("è®­ç»ƒè¢«å–æ¶ˆï¼Œè·³è¿‡é‡è¯•")
                    return False

                wait_msg = f"ç­‰å¾… {self._network_retry.retry_delay} ç§’åé‡è¯•..."
                log_info(wait_msg)
                if log_sink is not None:
                    log_sink.write_line(wait_msg, phase='train')
                if log_callback:
                    log_callback(wait_msg)

                # åˆ†æ®µç­‰å¾…ï¼Œæ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡å–æ¶ˆçŠ¶æ€
                for i in range(self._network_retry.retry_delay * 2):
                    if self._cancelled:
                        log_info("ç­‰å¾…é‡è¯•æœŸé—´è®­ç»ƒè¢«å–æ¶ˆ")
                        return False
                    time.sleep(0.5)

        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
        error_msg = f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯: {last_error}"
        if log_sink is not None:
            log_sink.write_line(error_msg, phase='train', level='error')
        else:
            self._emit_log(error_msg, "error")
        log_error(error_msg)
        if log_callback:
            log_callback(f"[å¤±è´¥] æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")

        return False

    def _monitor_training(self,
                          task: TrainingTask,
                          progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
                          log_callback: Optional[Callable[[str], None]] = None,
                          log_file=None,
                          log_sink: Optional[LogSink] = None) -> bool:
        """ç›‘æ§è®­ç»ƒè¿›åº¦ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼Œé˜²æ­¢å­è¿›ç¨‹é˜»å¡ï¼‰"""
        try:
            if not self._proc:
                return False

            error_lines = []  # æ”¶é›†é”™è¯¯è¾“å‡º
            
            # ğŸ”§ æ·»åŠ è¶…æ—¶ä¿æŠ¤æœºåˆ¶ï¼Œé˜²æ­¢ readline æ°¸ä¹…é˜»å¡
            import time
            last_output_time = time.time()
            no_output_timeout = 60  # 60ç§’æ— è¾“å‡ºåˆ™æ£€æŸ¥è¿›ç¨‹çŠ¶æ€

            while True:
                # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
                if self._cancelled:
                    log_info("è®­ç»ƒç›‘æ§è¿‡ç¨‹ä¸­æ”¶åˆ°å–æ¶ˆä¿¡å·")
                    if log_sink is not None:
                        log_sink.write_line("è®­ç»ƒè¢«å–æ¶ˆ", phase='train', level='warning')
                    else:
                        self._emit_log("è®­ç»ƒè¢«å–æ¶ˆ", "warning")
                    return False

                # ğŸ”§ è¶…æ—¶ä¿æŠ¤ï¼šå®šæœŸæ£€æŸ¥è¿›ç¨‹çŠ¶æ€ï¼Œé¿å…å› å­è¿›ç¨‹æŒæœ‰ç®¡é“è€Œæ°¸ä¹…é˜»å¡
                current_time = time.time()
                if current_time - last_output_time > no_output_timeout:
                    return_code = self._proc.poll()
                    if return_code is not None:
                        # è¿›ç¨‹å·²é€€å‡ºä½† readline å¯èƒ½è¢«å­è¿›ç¨‹é˜»å¡
                        log_info(f"æ£€æµ‹åˆ°è¿›ç¨‹å·²é€€å‡ºï¼ˆè¶…æ—¶ä¿æŠ¤è§¦å‘ï¼‰ï¼Œé€€å‡ºç : {return_code}")
                        break  # å¼ºåˆ¶é€€å‡ºç›‘æ§å¾ªç¯
                    else:
                        # è¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œé‡ç½®è®¡æ—¶å™¨ç»§ç»­ç­‰å¾…
                        log_info("è¿›ç¨‹ä»åœ¨è¿è¡Œä½†é•¿æ—¶é—´æ— è¾“å‡ºï¼Œç»§ç»­ç­‰å¾…...")
                        last_output_time = current_time

                output = self._proc.stdout.readline()
                if output == '' and self._proc.poll() is not None:
                    break

                if output:
                    last_output_time = time.time()  # ğŸ”§ é‡ç½®è¶…æ—¶è®¡æ—¶å™¨
                    line = output.strip()

                    # è¿‡æ»¤é‡å¤è¡Œï¼šä¸ä¸Šä¸€è¡Œå®Œå…¨ç›¸åŒåˆ™è·³è¿‡
                    if line == self._last_logged_line:
                        continue
                    
                    self._last_logged_line = line

                    # ç»Ÿä¸€å†™å…¥ä¸å¹¿æ’­
                    if log_sink is not None:
                        # ä½¿ç”¨ LogSink æ—¶ï¼Œæ‰€æœ‰æ—¥å¿—é€šè¿‡å…¶æ‰¹é‡æ¨é€æœºåˆ¶å¤„ç†ï¼ˆå·²åŒ…å«å»é‡ï¼‰
                        log_sink.write_line(line, phase='train')
                    else:
                        # ä¸ä½¿ç”¨ LogSink æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„æ–‡ä»¶å†™å…¥å’Œäº‹ä»¶æ¨é€
                        if log_file:
                            log_file.write(output)
                            log_file.flush()
                        self._emit_log(line)
                        # åªåœ¨ä¸ä½¿ç”¨ log_sink æ—¶æ‰è°ƒç”¨ callbackï¼ˆé¿å…é‡å¤æ¨é€ï¼‰
                        if log_callback:
                            log_callback(line)

                    # æ”¶é›†å¯èƒ½çš„é”™è¯¯ä¿¡æ¯
                    if self._network_retry._is_network_error(line):
                        error_lines.append(line)

                    # è§£æè®­ç»ƒè¿›åº¦
                    progress_info = self._parse_training_output(line)
                    if progress_info:
                        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                        for key, value in progress_info.items():
                            if hasattr(task, key):
                                setattr(task, key, value)

                        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
                        if task.total_steps > 0:
                            task.progress = task.current_step / task.total_steps

                        # å‘é€è¿›åº¦äº‹ä»¶å’Œå›è°ƒ
                        progress_data = {
                            "step": task.current_step,
                            "total_steps": task.total_steps,
                            "epoch": task.current_epoch,
                            "loss": task.loss,
                            "lr": task.learning_rate,
                            "speed": task.speed,
                            "eta_seconds": task.eta_seconds,
                            "progress": task.progress
                        }
                        self._emit_progress(**progress_data)

                        if progress_callback:
                            progress_callback(progress_data)

            # æ£€æŸ¥è®­ç»ƒç»“æœ
            return_code = self._proc.poll()
            log_info(f"è®­ç»ƒç›‘æ§ç»“æŸï¼Œè¿›ç¨‹é€€å‡ºç : {return_code}")
            
            if return_code == 0:
                success_msg = f"è®­ç»ƒå®Œæˆ: {task.name}"
                if log_sink is not None:
                    log_sink.write_line(success_msg, phase='train', level='success')
                else:
                    self._emit_log(success_msg, "success")
                log_success(success_msg)
                
                # ç¡®ä¿æœ€ç»ˆæ—¥å¿—èƒ½å¤Ÿå‘é€åˆ°å‰ç«¯
                time.sleep(0.2)  # ğŸ”§ ç¨å¾®å»¶é•¿ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿æ—¥å¿—å’ŒçŠ¶æ€éƒ½èƒ½æ¨é€

                if progress_callback:
                    progress_callback({"state": "completed"})
                return True
            else:
                # æ„å»ºé”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç½‘ç»œé”™è¯¯
                if error_lines:
                    error_msg = f"è®­ç»ƒå¤±è´¥ï¼Œç½‘ç»œé”™è¯¯: {'; '.join(error_lines[-3:])}"  # æœ€å3è¡Œé”™è¯¯
                else:
                    error_msg = f"è®­ç»ƒè¿›ç¨‹é€€å‡ºï¼Œä»£ç : {return_code}"

                if log_sink is not None:
                    log_sink.write_line(error_msg, phase='train', level='error')
                else:
                    self._emit_log(error_msg, "error")
                log_error(error_msg)
                
                # ç¡®ä¿æœ€ç»ˆæ—¥å¿—èƒ½å¤Ÿå‘é€åˆ°å‰ç«¯
                time.sleep(0.2)  # ğŸ”§ ç¨å¾®å»¶é•¿ç­‰å¾…æ—¶é—´
                
                if progress_callback:
                    progress_callback({
                        "state": "failed",
                        "error": error_msg
                    })
                return False

        except Exception as e:
            error_msg = str(e)
            if log_sink is not None:
                log_sink.write_line(f"è®­ç»ƒç›‘æ§å¤±è´¥: {error_msg}", phase='train', level='error')
            else:
                self._emit_log(f"è®­ç»ƒç›‘æ§å¤±è´¥: {error_msg}", "error")
            log_error(f"è®­ç»ƒç›‘æ§å¤±è´¥: {error_msg}")
            if progress_callback:
                progress_callback({
                    "state": "failed",
                    "error": error_msg
                })
            return False

    def _parse_training_output(self, line: str) -> Optional[Dict[str, Any]]:
        """è§£æè®­ç»ƒè¾“å‡ºï¼Œæå–è¿›åº¦ä¿¡æ¯"""
        try:
            progress_info = {}
            low = line.lower()

            # ä»…åœ¨æ˜ç¡®çš„è®­ç»ƒè¿›åº¦ä¸Šä¸‹æ–‡ä¸­è§£æï¼ˆé¿å…è¯¯åŒ¹é…æ¨¡å‹åŠ è½½è¿›åº¦ç­‰ï¼‰ï¼š
            has_steps_marker = ('steps:' in low) or bool(re.search(r'\bstep\s+\d+/\d+', line, re.IGNORECASE))
            has_epoch_marker = bool(re.search(r'\bepoch\s+\d+/\d+', line, re.IGNORECASE))

            # è§£ææ­¥æ•°å’Œè½®æ¬¡ï¼ˆå…¼å®¹å¤šç§æ ¼å¼ï¼‰
            # 1) Epoch E/Nï¼ˆç‹¬ç«‹å­˜åœ¨ä¹Ÿå…è®¸è§£æï¼‰
            epoch_match = re.search(r'Epoch\s+(\d+)/(\d+)', line, re.IGNORECASE)
            if epoch_match:
                progress_info['current_epoch'] = int(epoch_match.group(1))
                progress_info['total_epochs'] = int(epoch_match.group(2))

            # 2) Step X/Yï¼ˆæ˜¾ç¤ºçš„ Step å‰ç¼€ï¼‰
            step_match = re.search(r'Step\s+(\d+)/(\d+)', line, re.IGNORECASE)
            if step_match:
                progress_info['current_step'] = int(step_match.group(1))
                progress_info['total_steps'] = int(step_match.group(2))

            # 3) tqdm é£æ ¼çš„ "3/640 [...]"ï¼ˆä»…åœ¨å¸¦æœ‰ steps: æ ‡è®°çš„è¡Œå…œåº•ï¼‰
            if ('current_step' not in progress_info or 'total_steps' not in progress_info) and has_steps_marker:
                generic_step = re.search(r'(\d+)\s*/\s*(\d+)', line)
                if generic_step:
                    cs = int(generic_step.group(1))
                    ts = int(generic_step.group(2))
                    if ts >= cs and ts <= 10_000_000:
                        progress_info['current_step'] = cs
                        progress_info['total_steps'] = ts

            # è§£æloss
            # TB å·²èƒ½æä¾› lossï¼Œè¿™é‡Œä»…ä½œå…œåº•ï¼šå…¼å®¹ avr_loss æˆ– loss= / loss:
            loss_match = re.search(r'(?:avr_)?loss\s*[:=]\s*([\d.]+)', line, re.IGNORECASE)
            if loss_match:
                progress_info['loss'] = float(loss_match.group(1))

            # è§£æå­¦ä¹ ç‡
            lr_match = re.search(r'lr:?\s*([\d.e-]+)', line, re.IGNORECASE)
            if lr_match:
                progress_info['learning_rate'] = float(lr_match.group(1))

            # è§£æé€Ÿåº¦ï¼šä»…åœ¨è®­ç»ƒè¿›åº¦è¡Œä¸­è§£æï¼ˆé¿å…åŒ¹é…åŠ è½½è¡Œï¼‰
            if has_steps_marker or has_epoch_marker:
                speed_its = re.search(r'([\d.]+)\s*it/s', line, re.IGNORECASE)
                if speed_its:
                    progress_info['speed'] = float(speed_its.group(1))
                    progress_info['speed_unit'] = 'it/s'
                else:
                    speed_sit = re.search(r'([\d.]+)\s*s/it', line, re.IGNORECASE)
                    if speed_sit:
                        try:
                            v = float(speed_sit.group(1))
                            if v > 0:
                                progress_info['speed'] = v  # ç›´æ¥ä¿å­˜åŸå€¼ï¼Œä¸è½¬æ¢
                                progress_info['speed_unit'] = 's/it'
                        except Exception:
                            pass

            # è§£æETAï¼šä»…åœ¨è®­ç»ƒè¿›åº¦è¡Œä¸­è§£æ
            if has_steps_marker or has_epoch_marker:
                eta_match = re.search(r'ETA\s*:?\s*(\d{1,2}):(\d{2}):(\d{2})', line, re.IGNORECASE)
                if eta_match:
                    hours, minutes, seconds = map(int, eta_match.groups())
                    progress_info['eta_seconds'] = hours * 3600 + minutes * 60 + seconds
                else:
                    # æ”¯æŒ <HH:MM:SS æ ¼å¼
                    eta_angle = re.search(r'<\s*(\d{1,2}):(\d{2}):(\d{2})', line)
                    if eta_angle:
                        hours, minutes, seconds = map(int, eta_angle.groups())
                        progress_info['eta_seconds'] = hours * 3600 + minutes * 60 + seconds
                    else:
                        # æ”¯æŒ <MM:SS æ ¼å¼ï¼ˆæ— å°æ—¶éƒ¨åˆ†ï¼‰
                        eta_short = re.search(r'<\s*(\d{1,2}):(\d{2})\b', line)
                        if eta_short:
                            minutes, seconds = map(int, eta_short.groups())
                            progress_info['eta_seconds'] = minutes * 60 + seconds

            # æ¨å¯¼è¿›åº¦
            if 'current_step' in progress_info and 'total_steps' in progress_info and progress_info['total_steps']:
                try:
                    progress_info['progress'] = max(0.0, min(1.0, progress_info['current_step'] / progress_info['total_steps']))
                except Exception:
                    pass

            return progress_info if progress_info else None

        except Exception as e:
            log_error(f"è§£æè®­ç»ƒè¾“å‡ºå¤±è´¥: {str(e)}")
            return None

    def cancel_training(self):
        """å–æ¶ˆè®­ç»ƒ - å¼ºåˆ¶ç»ˆæ­¢æ‰€æœ‰ç›¸å…³è¿›ç¨‹"""
        self._cancelled = True  # è®¾ç½®å–æ¶ˆæ ‡å¿—

        # å¤„ç†é¢„å¤„ç†è¿›ç¨‹
        if self._cache_proc and self._cache_proc.poll() is None:
            try:
                log_info("æ­£åœ¨å–æ¶ˆé¢„å¤„ç†è¿›ç¨‹...")
                self._cache_proc.terminate()
                try:
                    self._cache_proc.wait(timeout=5)
                    log_info("é¢„å¤„ç†è¿›ç¨‹å·²ç»ˆæ­¢")
                except subprocess.TimeoutExpired:
                    log_info("é¢„å¤„ç†è¿›ç¨‹æœªå“åº”ï¼Œå¼ºåˆ¶ç»ˆæ­¢...")
                    self._cache_proc.kill()
                    self._cache_proc.wait()
                    log_info("é¢„å¤„ç†è¿›ç¨‹å·²å¼ºåˆ¶ç»ˆæ­¢")
                self._cache_proc = None
                return  # å¦‚æœåªæ˜¯é¢„å¤„ç†é˜¶æ®µï¼Œç›´æ¥è¿”å›
            except Exception as e:
                log_error(f"ç»ˆæ­¢é¢„å¤„ç†è¿›ç¨‹å¤±è´¥: {e}")

        # å¤„ç†ä¸»è®­ç»ƒè¿›ç¨‹
        if self._proc and self._proc.poll() is None:
            try:
                log_info("æ­£åœ¨å–æ¶ˆè®­ç»ƒ...")

                # è·å–ä¸»è¿›ç¨‹PID
                main_pid = self._proc.pid
                log_info(f"ä¸»è®­ç»ƒè¿›ç¨‹PID: {main_pid}")

                # æ–¹æ³•1: å°è¯•ä¼˜é›…ç»ˆæ­¢è¿›ç¨‹æ ‘
                try:
                    parent = psutil.Process(main_pid)
                    children = parent.children(recursive=True)

                    log_info(f"å‘ç° {len(children)} ä¸ªå­è¿›ç¨‹")

                    # é¦–å…ˆå°è¯•ä¼˜é›…ç»ˆæ­¢æ‰€æœ‰å­è¿›ç¨‹ï¼ˆä»æœ€æ·±çš„å­è¿›ç¨‹å¼€å§‹ï¼‰
                    for child in reversed(children):  # åå‘éå†ï¼Œå…ˆæ€æœ€æ·±çš„å­è¿›ç¨‹
                        try:
                            log_info(f"ç»ˆæ­¢å­è¿›ç¨‹: PID={child.pid}, åç§°={child.name()}")
                            child.terminate()
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            pass

                    # ç»ˆæ­¢ä¸»è¿›ç¨‹
                    parent.terminate()

                    # ç­‰å¾…è¿›ç¨‹ç»“æŸ
                    gone, alive = psutil.wait_procs(children + [parent], timeout=5)

                    # å¼ºåˆ¶æ€æ­»ä»ç„¶å­˜æ´»çš„è¿›ç¨‹
                    if alive:
                        log_info(f"å¼ºåˆ¶æ€æ­» {len(alive)} ä¸ªæœªå“åº”çš„è¿›ç¨‹")
                        for proc in alive:
                            try:
                                log_info(f"å¼ºåˆ¶æ€æ­»è¿›ç¨‹: PID={proc.pid}, åç§°={proc.name()}")
                                proc.kill()
                            except (psutil.NoSuchProcess, psutil.AccessDenied):
                                pass

                        # å†æ¬¡ç­‰å¾…ï¼Œç¼©çŸ­è¶…æ—¶æ—¶é—´
                        psutil.wait_procs(alive, timeout=2)

                except psutil.NoSuchProcess:
                    log_info("ä¸»è¿›ç¨‹å·²ä¸å­˜åœ¨")
                except Exception as e:
                    log_error(f"ä½¿ç”¨psutilç»ˆæ­¢è¿›ç¨‹å¤±è´¥: {e}")

                    # æ–¹æ³•2: å›é€€åˆ°åŸå§‹çš„è¿›ç¨‹ç»ˆæ­¢æ–¹æ³•
                    log_info("å›é€€åˆ°åŸºç¡€è¿›ç¨‹ç»ˆæ­¢æ–¹æ³•")
                    try:
                        if os.name == 'nt':  # Windows
                            # Windowsä¸Šå¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹æ ‘
                            subprocess.run([
                                "taskkill", "/F", "/T", "/PID", str(main_pid)
                            ], capture_output=True, check=False)
                        else:  # Unix/Linux
                            # å‘é€SIGTERMåˆ°è¿›ç¨‹ç»„
                            os.killpg(os.getpgid(main_pid), signal.SIGTERM)
                            time.sleep(2)
                            # å¦‚æœè¿˜æœªç»“æŸï¼Œå‘é€SIGKILL
                            try:
                                os.killpg(os.getpgid(main_pid), signal.SIGKILL)
                            except ProcessLookupError:
                                pass
                    except Exception as e2:
                        log_error(f"å›é€€æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")

                # æ–¹æ³•3: é¢å¤–å®‰å…¨æ£€æŸ¥ - æŸ¥æ‰¾å¯èƒ½çš„è®­ç»ƒç›¸å…³è¿›ç¨‹
                try:
                    self._cleanup_training_processes()
                except Exception as e:
                    log_error(f"æ¸…ç†è®­ç»ƒè¿›ç¨‹æ—¶å‡ºé”™: {e}")

                log_info("è®­ç»ƒå–æ¶ˆå®Œæˆ")

            except Exception as e:
                log_error(f"å–æ¶ˆè®­ç»ƒæ—¶å‡ºé”™: {e}")
            finally:
                self._proc = None

    def _cleanup_training_processes(self):
        """æ¸…ç†å¯èƒ½æ®‹ç•™çš„è®­ç»ƒç›¸å…³è¿›ç¨‹"""
        try:
            # æŸ¥æ‰¾å¯èƒ½çš„è®­ç»ƒè¿›ç¨‹ï¼ˆåŸºäºè¿›ç¨‹åç§°å’Œå‘½ä»¤è¡Œï¼‰
            training_keywords = [
                "qwen_image_train_network.py",
                "musubi_tuner",
                "accelerate",
                "torch.distributed.run"
            ]

            killed_count = 0
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    cmdline = ' '.join(proc.info['cmdline'] or [])

                    # æ£€æŸ¥æ˜¯å¦ä¸ºè®­ç»ƒç›¸å…³è¿›ç¨‹
                    is_training_proc = any(keyword in cmdline.lower() for keyword in training_keywords)

                    if is_training_proc:
                        # é¢å¤–æ£€æŸ¥ç¡®ä¿ä¸æ˜¯å½“å‰Pythonè§£é‡Šå™¨è¿›ç¨‹
                        if proc.pid != os.getpid():
                            log_info(f"å‘ç°å¯èƒ½çš„æ®‹ç•™è®­ç»ƒè¿›ç¨‹: PID={proc.pid}, å‘½ä»¤={cmdline[:100]}...")
                            proc.kill()
                            killed_count += 1

                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                    pass

            if killed_count > 0:
                log_info(f"æ¸…ç†äº† {killed_count} ä¸ªæ®‹ç•™çš„è®­ç»ƒè¿›ç¨‹")

        except Exception as e:
            log_error(f"æ¸…ç†è¿›ç¨‹æ—¶å‡ºé”™: {e}")

    def _emergency_cleanup(self):
        """ç¨‹åºé€€å‡ºæ—¶çš„ç´§æ€¥æ¸…ç†"""
        try:
            if self._proc and self._proc.poll() is None:
                log_info("ç¨‹åºé€€å‡ºæ—¶å‘ç°æ­£åœ¨è¿è¡Œçš„è®­ç»ƒï¼Œæ‰§è¡Œç´§æ€¥æ¸…ç†")
                self.cancel_training()

            # æ¸…ç†ç½‘ç»œé‡è¯•åŠ©æ‰‹
            if self._network_retry:
                self._network_retry.cleanup()
        except Exception as e:
            # é™é»˜å¤„ç†ï¼Œé¿å…ç¨‹åºé€€å‡ºæ—¶å‡ºç°é”™è¯¯
            pass

    def is_available(self) -> bool:
        """æ£€æŸ¥Musubi-Tuneræ˜¯å¦å¯ç”¨ï¼ˆworkspace-based runtimeï¼‰"""
        try:
            # âœ¨ ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ musubi_dirï¼ˆworkspace/runtime/engines/musubi-tunerï¼‰
            if not self._musubi_dir.exists():
                return False

            # æ£€æŸ¥å…³é”®è„šæœ¬æ˜¯å¦å­˜åœ¨
            for model_spec in list_models():
                script_rel_path = f"src/musubi_tuner/{model_spec.script_train}"
                script_path = self._musubi_dir / script_rel_path
                if not script_path.exists():
                    return False

            return True

        except Exception:
            return False

    def _generate_dataset_config(self, toml_path: Path, dataset_id: str, config: BaseTrainingConfig):
        """ç”Ÿæˆæ•°æ®é›†é…ç½®æ–‡ä»¶ (ä¾›TrainingManagerè°ƒç”¨)"""
        try:
            # è®¡ç®—æ•°æ®é›†è·¯å¾„
            dataset_path = self._resolve_dataset_path(dataset_id)

            # è®¡ç®—ç¼“å­˜ç›®å½• (ç›¸å¯¹äºtomlæ–‡ä»¶çš„ä½ç½®)
            cache_dir = toml_path.parent / "cache"

            # è®¾ç½®è·¯å¾„åˆ°é…ç½®ä¸­
            config.image_video_directory = self._rel_to_root_posix(dataset_path)
            config.cache_directory = self._rel_to_root_posix(cache_dir)

            # ç”ŸæˆTOMLå†…å®¹
            toml_dict = build_toml_dict(config)
            toml_content = dumps_toml(toml_dict)

            # å†™å…¥æ–‡ä»¶
            toml_path.write_text(toml_content, encoding="utf-8")

        except Exception as e:
            log_error(f"ç”Ÿæˆæ•°æ®é›†é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
